{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eb961520-81e7-da2c-5fa7-852f98749f3e"
   },
   "source": [
    "# Introduction\n",
    "## Overview\n",
    "The notebook applies the [SimGAN](https://arxiv.org/pdf/1612.07828v1.pdf) network architecture to the problem of generating realistic images of eyes by using real images to augment the simulated data. We use Keras with a Tensorflow-backend to accomplish the adversarial training. \n",
    "### Note \n",
    "The model is quite small and the training is tuned down substantially since the limits of Kaggle Kernels and the lack of GPU makes the training very slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "bb0b3648-a55b-e9dd-7396-97fbedf4a3c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import keras\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "d395fcbf-7846-00df-e949-728c55e5528b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tf-version', '1.6.0', 'keras-version', '2.2.4')\n"
     ]
    }
   ],
   "source": [
    "print('tf-version',tf.__version__, 'keras-version', keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "de81853c-aaff-d745-0f05-57c6d89a1038"
   },
   "source": [
    "## Loading Data\n",
    "Here we setup the paths and load the data from the hdf5 files since there would otherwise be too many individual jpg/png images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "c4d71194-04e2-49f0-ad43-d4297fccf7b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/home/tjb2147', '\\n', '../simgan_input', '\\n', '.')\n",
      "[u'image', u'look_vec', u'path']\n",
      "('Synthetic images found:', 50000)\n",
      "('image', u'..\\\\..\\\\..\\\\..\\\\Downloads\\\\UnityEyes_Windows\\\\UnityEyes_Windows\\\\imgs\\\\1.jpg', 'shape:', (35, 55))\n",
      "[u'image']\n",
      "('Real Images found:', 34100)\n",
      "('image', u'..\\\\data\\\\MPIIGaze_Dataset\\\\000055ed-890a-411f-be6e-b8453b495389.png', 'shape:', (35, 55))\n"
     ]
    }
   ],
   "source": [
    "path = os.path.dirname(os.path.abspath('.'))\n",
    "data_dir = os.path.join('..', 'simgan_input')\n",
    "cache_dir = '.'\n",
    "print(path, \"\\n\", data_dir, \"\\n\", cache_dir)\n",
    "# load the data file and extract dimensions\n",
    "with h5py.File(os.path.join(data_dir,'gaze.h5'),'r') as t_file:\n",
    "    print(list(t_file.keys()))\n",
    "    assert 'image' in t_file, \"Images are missing\"\n",
    "    assert 'look_vec' in t_file, \"Look vector is missing\"\n",
    "    assert 'path' in t_file, \"Paths are missing\"\n",
    "    print('Synthetic images found:',len(t_file['image']))\n",
    "    for _, (ikey, ival) in zip(range(1), t_file['image'].items()):\n",
    "        print('image',ikey,'shape:',ival.shape)\n",
    "        img_height, img_width = ival.shape\n",
    "        img_channels = 1\n",
    "    syn_image_stack = np.stack([np.expand_dims(a,-1) for a in t_file['image'].values()],0)\n",
    "\n",
    "with h5py.File(os.path.join(data_dir,'real_gaze.h5'),'r') as t_file:\n",
    "    print(list(t_file.keys()))\n",
    "    assert 'image' in t_file, \"Images are missing\"\n",
    "    print('Real Images found:',len(t_file['image']))\n",
    "    for _, (ikey, ival) in zip(range(1), t_file['image'].items()):\n",
    "        print('image',ikey,'shape:',ival.shape)\n",
    "        img_height, img_width = ival.shape\n",
    "        img_channels = 1\n",
    "    real_image_stack = np.stack([np.expand_dims(a,-1) for a in t_file['image'].values()],0)\n",
    "\n",
    "#\n",
    "# training params\n",
    "#\n",
    "\n",
    "nb_steps = 20 # originally 10000, but this makes the kernel time out\n",
    "batch_size = 25 # 25 works on GPU\n",
    "k_d = 1  # number of discriminator updates per step\n",
    "k_g = 2  # number of generative network updates per step\n",
    "log_interval = 100\n",
    "pre_steps = 15 # for pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4881321d-64f8-fe2a-d8bb-1c74500754a4"
   },
   "source": [
    "## Utility Functions\n",
    "These functions make it a bit easier to keep track of how the training is going and make it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "da7514ef-2b69-8945-79da-fb0583dd36ea"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Module to plot a batch of images along w/ their corresponding label(s)/annotations and save the plot to disc.\n",
    "\n",
    "Use cases:\n",
    "Plot images along w/ their corresponding ground-truth label & model predicted label,\n",
    "Plot images generated by a GAN along w/ any annotations used to generate these images,\n",
    "Plot synthetic, generated, refined, and real images and see how they compare as training progresses in a GAN,\n",
    "etc...\n",
    "\"\"\"\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from itertools import groupby\n",
    "from skimage.util import montage\n",
    "def plot_batch(image_batch, figure_path, label_batch=None):\n",
    "    \n",
    "    all_groups = {label: montage(np.stack([img[:,:,0] for img, lab in img_lab_list],0)) \n",
    "                  for label, img_lab_list in groupby(zip(image_batch, label_batch), lambda x: x[1])}\n",
    "    fig, c_axs = plt.subplots(1,len(all_groups), figsize=(len(all_groups)*4, 8), dpi = 600)\n",
    "    for c_ax, (c_label, c_mtg) in zip(c_axs, all_groups.items()):\n",
    "        c_ax.imshow(c_mtg, cmap='bone')\n",
    "        c_ax.set_title(c_label)\n",
    "        c_ax.axis('off')\n",
    "    fig.savefig(os.path.join(figure_path))\n",
    "    plt.close()\n",
    "\"\"\"\n",
    "Module implementing the image history buffer described in `2.3. Updating Discriminator using a History of\n",
    "Refined Images` of https://arxiv.org/pdf/1612.07828v1.pdf.\n",
    "\n",
    "\"\"\"\n",
    "class ImageHistoryBuffer(object):\n",
    "    def __init__(self, shape, max_size, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the class's state.\n",
    "\n",
    "        :param shape: Shape of the data to be stored in the image history buffer\n",
    "                      (i.e. (0, img_height, img_width, img_channels)).\n",
    "        :param max_size: Maximum number of images that can be stored in the image history buffer.\n",
    "        :param batch_size: Batch size used to train GAN.\n",
    "        \"\"\"\n",
    "        self.image_history_buffer = np.zeros(shape=shape)\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add_to_image_history_buffer(self, images, nb_to_add=None):\n",
    "        \"\"\"\n",
    "        To be called during training of GAN. By default add batch_size // 2 images to the image history buffer each\n",
    "        time the generator generates a new batch of images.\n",
    "\n",
    "        :param images: Array of images (usually a batch) to be added to the image history buffer.\n",
    "        :param nb_to_add: The number of images from `images` to add to the image history buffer\n",
    "                          (batch_size / 2 by default).\n",
    "        \"\"\"\n",
    "        if not nb_to_add:\n",
    "            nb_to_add = self.batch_size // 2\n",
    "\n",
    "        if len(self.image_history_buffer) < self.max_size:\n",
    "            np.append(self.image_history_buffer, images[:nb_to_add], axis=0)\n",
    "        elif len(self.image_history_buffer) == self.max_size:\n",
    "            self.image_history_buffer[:nb_to_add] = images[:nb_to_add]\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        np.random.shuffle(self.image_history_buffer)\n",
    "\n",
    "    def get_from_image_history_buffer(self, nb_to_get=None):\n",
    "        \"\"\"\n",
    "        Get a random sample of images from the history buffer.\n",
    "\n",
    "        :param nb_to_get: Number of images to get from the image history buffer (batch_size / 2 by default).\n",
    "        :return: A random sample of `nb_to_get` images from the image history buffer, or an empty np array if the image\n",
    "                 history buffer is empty.\n",
    "        \"\"\"\n",
    "        if not nb_to_get:\n",
    "            nb_to_get = self.batch_size // 2\n",
    "\n",
    "        try:\n",
    "            return self.image_history_buffer[:nb_to_get]\n",
    "        except IndexError:\n",
    "            return np.zeros(shape=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e8e42268-05b1-0e33-9fe1-f02e0a6d234e"
   },
   "source": [
    "# Network Architectures\n",
    "Here we define the two primary networks\n",
    "\n",
    " - refiner network\n",
    " - discriminator network\n",
    "\n",
    "They work against each other to constantly improve the results of the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "701fd9e1-f970-8420-504b-7c057e0173a2"
   },
   "outputs": [],
   "source": [
    "def refiner_network(input_image_tensor):\n",
    "    \"\"\"\n",
    "    The refiner network, RÎ¸, is a residual network (ResNet). It modifies the synthetic image on a pixel level, rather\n",
    "    than holistically modifying the image content, preserving the global structure and annotations.\n",
    "\n",
    "    :param input_image_tensor: Input tensor that corresponds to a synthetic image.\n",
    "    :return: Output tensor that corresponds to a refined synthetic image.\n",
    "    \"\"\"\n",
    "    def resnet_block(input_features, nb_features=64, nb_kernel_rows=3, nb_kernel_cols=3):\n",
    "        \"\"\"\n",
    "        A ResNet block with two `nb_kernel_rows` x `nb_kernel_cols` convolutional layers,\n",
    "        each with `nb_features` feature maps.\n",
    "\n",
    "        See Figure 6 in https://arxiv.org/pdf/1612.07828v1.pdf.\n",
    "\n",
    "        :param input_features: Input tensor to ResNet block.\n",
    "        :return: Output tensor from ResNet block.\n",
    "        \"\"\"\n",
    "        y = layers.Convolution2D(nb_features, nb_kernel_rows, nb_kernel_cols, border_mode='same')(input_features)\n",
    "        y = layers.Activation('relu')(y)\n",
    "        y = layers.Convolution2D(nb_features, nb_kernel_rows, nb_kernel_cols, border_mode='same')(y)\n",
    "\n",
    "#         y = layers.merge([input_features, y], mode='sum')\n",
    "        y = layers.add([input_features, y])\n",
    "        return layers.Activation('relu')(y)\n",
    "\n",
    "    # an input image of size w Ã h is convolved with 3 Ã 3 filters that output 64 feature maps\n",
    "    x = layers.Convolution2D(64, 3, 3, border_mode='same', activation='relu')(input_image_tensor)\n",
    "\n",
    "    # the output is passed through 4 ResNet blocks\n",
    "    for _ in range(4):\n",
    "        x = resnet_block(x)\n",
    "\n",
    "    # the output of the last ResNet block is passed to a 1 Ã 1 convolutional layer producing 1 feature map\n",
    "    # corresponding to the refined synthetic image\n",
    "    return layers.Convolution2D(img_channels, 1, 1, border_mode='same', activation='tanh')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "35420452-e116-f91a-a386-c2f8f290effe"
   },
   "outputs": [],
   "source": [
    "def discriminator_network(input_image_tensor):\n",
    "    \"\"\"\n",
    "    The discriminator network, DÏ, contains 5 convolution layers and 2 max-pooling layers.\n",
    "\n",
    "    :param input_image_tensor: Input tensor corresponding to an image, either real or refined.\n",
    "    :return: Output tensor that corresponds to the probability of whether an image is real or refined.\n",
    "    \"\"\"\n",
    "    x = layers.Convolution2D(96, 3, 3, border_mode='same', subsample=(2, 2), activation='relu')(input_image_tensor)\n",
    "    x = layers.Convolution2D(64, 3, 3, border_mode='same', subsample=(2, 2), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), border_mode='same', strides=(1, 1))(x)\n",
    "    x = layers.Convolution2D(32, 3, 3, border_mode='same', subsample=(1, 1), activation='relu')(x)\n",
    "    x = layers.Convolution2D(32, 1, 1, border_mode='same', subsample=(1, 1), activation='relu')(x)\n",
    "    x = layers.Convolution2D(2, 1, 1, border_mode='same', subsample=(1, 1), activation='relu')(x)\n",
    "\n",
    "    # here one feature map corresponds to `is_real` and the other to `is_refined`,\n",
    "    # and the custom loss function is then `tf.nn.sparse_softmax_cross_entropy_with_logits`\n",
    "    return layers.Reshape((-1, 2))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "365c0d27-8e84-1561-2434-3bde90c31534"
   },
   "source": [
    "## Combining Models\n",
    "Adversarial training of refiner network RÎ¸ and discriminator network DÏ and combining them into single loss-functions and models that can be trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "fe1d7259-a48d-bb66-e471-635ac4958124"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tjb2147/.local/lib/python2.7/site-packages/ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/home/tjb2147/.local/lib/python2.7/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\")`\n",
      "/home/tjb2147/.local/lib/python2.7/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\")`\n",
      "/home/tjb2147/.local/lib/python2.7/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (1, 1), padding=\"same\", activation=\"tanh\")`\n",
      "/home/tjb2147/.local/lib/python2.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3), padding=\"same\", strides=(2, 2), activation=\"relu\")`\n",
      "  \n",
      "/home/tjb2147/.local/lib/python2.7/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(2, 2), activation=\"relu\")`\n",
      "  if __name__ == '__main__':\n",
      "/home/tjb2147/.local/lib/python2.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(padding=\"same\", strides=(1, 1), pool_size=(3, 3))`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/tjb2147/.local/lib/python2.7/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", strides=(1, 1), activation=\"relu\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/tjb2147/.local/lib/python2.7/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), padding=\"same\", strides=(1, 1), activation=\"relu\")`\n",
      "  if sys.path[0] == '':\n",
      "/home/tjb2147/.local/lib/python2.7/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2, (1, 1), padding=\"same\", strides=(1, 1), activation=\"relu\")`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 35, 55, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 35, 55, 64)   640         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 35, 55, 64)   36928       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 35, 55, 64)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 35, 55, 64)   36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 35, 55, 64)   0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 35, 55, 64)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 35, 55, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 35, 55, 64)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 35, 55, 64)   36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 35, 55, 64)   0           activation_2[0][0]               \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 35, 55, 64)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 35, 55, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 35, 55, 64)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 35, 55, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 35, 55, 64)   0           activation_4[0][0]               \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 35, 55, 64)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 35, 55, 64)   36928       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 35, 55, 64)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 35, 55, 64)   36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 35, 55, 64)   0           activation_6[0][0]               \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 35, 55, 64)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 35, 55, 1)    65          activation_8[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 296,129\n",
      "Trainable params: 296,129\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 35, 55, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 18, 28, 96)        960       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 9, 14, 64)         55360     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 14, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 9, 14, 32)         18464     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 9, 14, 32)         1056      \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 9, 14, 2)          66        \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 126, 2)            0         \n",
      "=================================================================\n",
      "Total params: 75,906\n",
      "Trainable params: 75,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35, 55, 1)         0         \n",
      "_________________________________________________________________\n",
      "refiner (Model)              (None, 35, 55, 1)         296129    \n",
      "_________________________________________________________________\n",
      "discriminator (Model)        (None, 126, 2)            75906     \n",
      "=================================================================\n",
      "Total params: 372,035\n",
      "Trainable params: 372,035\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tjb2147/.local/lib/python2.7/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"co..., name=\"refiner\", inputs=Tensor(\"in...)`\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/tjb2147/.local/lib/python2.7/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"re..., name=\"discriminator\", inputs=Tensor(\"in...)`\n",
      "/home/tjb2147/.local/lib/python2.7/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., name=\"combined\", inputs=Tensor(\"in...)`\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# define model input and output tensors\n",
    "#\n",
    "\n",
    "synthetic_image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "refined_image_tensor = refiner_network(synthetic_image_tensor)\n",
    "\n",
    "refined_or_real_image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "discriminator_output = discriminator_network(refined_or_real_image_tensor)\n",
    "\n",
    "#\n",
    "# define models\n",
    "#\n",
    "\n",
    "refiner_model = models.Model(input=synthetic_image_tensor, output=refined_image_tensor, name='refiner')\n",
    "discriminator_model = models.Model(input=refined_or_real_image_tensor, output=discriminator_output,\n",
    "                                   name='discriminator')\n",
    "\n",
    "# combined must output the refined image along w/ the disc's classification of it for the refiner's self-reg loss\n",
    "refiner_model_output = refiner_model(synthetic_image_tensor)\n",
    "combined_output = discriminator_model(refiner_model_output)\n",
    "combined_model = models.Model(input=synthetic_image_tensor, output=[refiner_model_output, combined_output],\n",
    "                              name='combined')\n",
    "\n",
    "discriminator_model_output_shape = discriminator_model.output_shape\n",
    "\n",
    "print(refiner_model.summary())\n",
    "print(discriminator_model.summary())\n",
    "print(combined_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "33ede9dc-63dc-e7eb-cabf-39ac01fcd5f4"
   },
   "source": [
    "## Visualization\n",
    "Here we show the network architectures visually with shapes (doesn't yet work on kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "44bc1cfc-a7f4-1aa4-432f-bef7473ff3a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running the patched version of keras/pydot!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "# Define model\n",
    "try:\n",
    "    model_to_dot(refiner_model, show_shapes=True).write_svg('refiner_model.svg')\n",
    "    SVG('refiner_model.svg')\n",
    "except ImportError:\n",
    "    print('Not running the patched version of keras/pydot!')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "c2821044-015b-5b2e-4cf7-b22195158767"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model_to_dot(discriminator_model, show_shapes=True).write_svg('discriminator_model.svg')\n",
    "    SVG('discriminator_model.svg')\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "e7446d98-534f-1094-ee09-245f3f7d28f7"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# define custom l1 loss function for the refiner\n",
    "#\n",
    "\n",
    "def self_regularization_loss(y_true, y_pred):\n",
    "    delta = 0.0001  # FIXME: need to figure out an appropriate value for this\n",
    "    return tf.multiply(delta, tf.reduce_sum(tf.abs(y_pred - y_true)))\n",
    "\n",
    "#\n",
    "# define custom local adversarial loss (softmax for each image section) for the discriminator\n",
    "# the adversarial loss function is the sum of the cross-entropy losses over the local patches\n",
    "#\n",
    "\n",
    "def local_adversarial_loss(y_true, y_pred):\n",
    "    # y_true and y_pred have shape (batch_size, # of local patches, 2), but really we just want to average over\n",
    "    # the local patches and batch size so we can reshape to (batch_size * # of local patches, 2)\n",
    "    y_true = tf.reshape(y_true, (-1, 2))\n",
    "    y_pred = tf.reshape(y_pred, (-1, 2))\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c70b7cb-dd4a-9a46-15f3-02d78fdc53f7"
   },
   "source": [
    "# Compile Models\n",
    "Here we combine the models and compile them with the loss functions defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "3faaf71b-0cfc-59e8-4b37-13ac79d61cfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-afb1781a18f1>:19: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd = optimizers.SGD(lr=1e-3)\n",
    "\n",
    "refiner_model.compile(optimizer=sgd, loss=self_regularization_loss)\n",
    "discriminator_model.compile(optimizer=sgd, loss=local_adversarial_loss)\n",
    "discriminator_model.trainable = True\n",
    "combined_model.compile(optimizer=sgd, loss=[self_regularization_loss, local_adversarial_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "d50c46c7-94cf-ba72-ab47-f9dd0a8f36c3"
   },
   "outputs": [],
   "source": [
    "refiner_model_path = None\n",
    "discriminator_model_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cb536531-09b4-d9b7-a23e-f3a82517939f"
   },
   "source": [
    "# Data Generators\n",
    "Here we setup the pipeline to feed new images to both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "ba8320cb-5aef-f682-0e95-48fd2a319ea5"
   },
   "outputs": [],
   "source": [
    "datagen = image.ImageDataGenerator(\n",
    "    preprocessing_function=applications.xception.preprocess_input,\n",
    "    data_format='channels_last')\n",
    "\n",
    "flow_from_directory_params = {'target_size': (img_height, img_width),\n",
    "                              'color_mode': 'grayscale' if img_channels == 1 else 'rgb',\n",
    "                              'class_mode': None,\n",
    "                              'batch_size': batch_size}\n",
    "flow_params = {'batch_size': batch_size}\n",
    "\n",
    "synthetic_generator = datagen.flow(\n",
    "    x = syn_image_stack,\n",
    "    **flow_params\n",
    ")\n",
    "\n",
    "real_generator = datagen.flow(\n",
    "    x = real_image_stack,\n",
    "    **flow_params\n",
    ")\n",
    "\n",
    "def get_image_batch(generator):\n",
    "    \"\"\"keras generators may generate an incomplete batch for the last batch\"\"\"\n",
    "    img_batch = generator.next()\n",
    "    if len(img_batch) != batch_size:\n",
    "        img_batch = generator.next()\n",
    "\n",
    "    assert len(img_batch) == batch_size\n",
    "\n",
    "    return img_batch\n",
    "\n",
    "# the target labels for the cross-entropy loss layer are 0 for every yj (real) and 1 for every xi (refined)\n",
    "y_real = np.array([[[1.0, 0.0]] * discriminator_model_output_shape[1]] * batch_size)\n",
    "y_refined = np.array([[[0.0, 1.0]] * discriminator_model_output_shape[1]] * batch_size)\n",
    "assert y_real.shape == (batch_size, discriminator_model_output_shape[1], 2)\n",
    "batch_out = get_image_batch(synthetic_generator)\n",
    "assert batch_out.shape == (batch_size, img_height, img_width, img_channels), \"Image Dimensions do not match, {}!={}\".format(batch_out.shape, (batch_size, img_height, img_width, img_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3670cae4-d254-3648-4a86-ac7e1e585ff2"
   },
   "source": [
    "# Pretraining\n",
    "Here we pretraining the models before we start the full-blown training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "22ceec0a-0697-d1ce-05b5-0cb68851edbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-training the refiner network...\n",
      "Saving batch of refined images during pre-training at step: 0.\n",
      "Refiner model self regularization loss: [0.01849633].\n",
      "pre-training the discriminator network...\n",
      "Discriminator model loss: [0.10355197].\n"
     ]
    }
   ],
   "source": [
    "if not refiner_model_path:\n",
    "    # we first train the RÎ¸ network with just self-regularization loss for 1,000 steps\n",
    "    print('pre-training the refiner network...')\n",
    "    gen_loss = np.zeros(shape=len(refiner_model.metrics_names))\n",
    "\n",
    "    for i in range(pre_steps):\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "        gen_loss = np.add(refiner_model.train_on_batch(synthetic_image_batch, synthetic_image_batch), gen_loss)\n",
    "\n",
    "        # log every `log_interval` steps\n",
    "        if not i % log_interval:\n",
    "            figure_name = 'refined_image_batch_pre_train_step_{}.png'.format(i)\n",
    "            print('Saving batch of refined images during pre-training at step: {}.'.format(i))\n",
    "\n",
    "            synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "            plot_batch(\n",
    "                np.concatenate((synthetic_image_batch, refiner_model.predict_on_batch(synthetic_image_batch))),\n",
    "                os.path.join(cache_dir, figure_name),\n",
    "                label_batch=['Synthetic'] * batch_size + ['Refined'] * batch_size)\n",
    "\n",
    "            print('Refiner model self regularization loss: {}.'.format(gen_loss / log_interval))\n",
    "            gen_loss = np.zeros(shape=len(refiner_model.metrics_names))\n",
    "\n",
    "    refiner_model.save(os.path.join(cache_dir, 'refiner_model_pre_trained.h5'))\n",
    "else:\n",
    "    refiner_model.load_weights(refiner_model_path)\n",
    "\n",
    "if not discriminator_model_path:\n",
    "    # and DÏ for 200 steps (one mini-batch for refined images, another for real)\n",
    "    print('pre-training the discriminator network...')\n",
    "    disc_loss = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "\n",
    "    for _ in range(pre_steps):\n",
    "        real_image_batch = get_image_batch(real_generator)\n",
    "        disc_loss = np.add(discriminator_model.train_on_batch(real_image_batch, y_real), disc_loss)\n",
    "\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "        refined_image_batch = refiner_model.predict_on_batch(synthetic_image_batch)\n",
    "        disc_loss = np.add(discriminator_model.train_on_batch(refined_image_batch, y_refined), disc_loss)\n",
    "\n",
    "    discriminator_model.save(os.path.join(cache_dir, 'discriminator_model_pre_trained.h5'))\n",
    "\n",
    "    # hard-coded for now\n",
    "    print('Discriminator model loss: {}.'.format(disc_loss / (100 * 2)))\n",
    "else:\n",
    "    discriminator_model.load_weights(discriminator_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4f7acaeb-5a0e-d418-b855-b3e85b7854c0"
   },
   "source": [
    "# Full Training\n",
    "Here we run the full training of the model (normally for thousands of iterations, but here just a few). The images are saved in the output directory for examining the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "6d7fb313-ec78-659b-0510-b6a235400294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 of 20.\n",
      "Saving batch of refined images at adversarial step: 0.\n",
      "Refiner model loss: [0.01266635 0.00580059 0.00686576].\n",
      "Discriminator model loss real: [0.00339249].\n",
      "Discriminator model loss refined: [0.00351127].\n",
      "Step: 1 of 20.\n",
      "Step: 2 of 20.\n",
      "Step: 3 of 20.\n",
      "Step: 4 of 20.\n",
      "Step: 5 of 20.\n",
      "Step: 6 of 20.\n",
      "Step: 7 of 20.\n",
      "Step: 8 of 20.\n",
      "Step: 9 of 20.\n",
      "Step: 10 of 20.\n",
      "Step: 11 of 20.\n",
      "Step: 12 of 20.\n",
      "Step: 13 of 20.\n",
      "Step: 14 of 20.\n",
      "Step: 15 of 20.\n",
      "Step: 16 of 20.\n",
      "Step: 17 of 20.\n",
      "Step: 18 of 20.\n",
      "Step: 19 of 20.\n"
     ]
    }
   ],
   "source": [
    "# TODO: what is an appropriate size for the image history buffer?\n",
    "image_history_buffer = ImageHistoryBuffer((0, img_height, img_width, img_channels), batch_size * 100, batch_size)\n",
    "\n",
    "combined_loss = np.zeros(shape=len(combined_model.metrics_names))\n",
    "disc_loss_real = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "disc_loss_refined = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "\n",
    "# see Algorithm 1 in https://arxiv.org/pdf/1612.07828v1.pdf\n",
    "for i in range(nb_steps):\n",
    "    print('Step: {} of {}.'.format(i, nb_steps))\n",
    "\n",
    "    # train the refiner\n",
    "    for _ in range(k_g * 2):\n",
    "        # sample a mini-batch of synthetic images\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "\n",
    "        # update Î¸ by taking an SGD step on mini-batch loss LR(Î¸)\n",
    "        combined_loss = np.add(combined_model.train_on_batch(synthetic_image_batch,\n",
    "                                                             [synthetic_image_batch, y_real]), combined_loss)\n",
    "\n",
    "    for _ in range(k_d):\n",
    "        # sample a mini-batch of synthetic and real images\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "        real_image_batch = get_image_batch(real_generator)\n",
    "\n",
    "        # refine the synthetic images w/ the current refiner\n",
    "        refined_image_batch = refiner_model.predict_on_batch(synthetic_image_batch)\n",
    "\n",
    "        # use a history of refined images\n",
    "        half_batch_from_image_history = image_history_buffer.get_from_image_history_buffer()\n",
    "        image_history_buffer.add_to_image_history_buffer(refined_image_batch)\n",
    "\n",
    "        if len(half_batch_from_image_history):\n",
    "            refined_image_batch[:batch_size // 2] = half_batch_from_image_history\n",
    "\n",
    "        # update Ï by taking an SGD step on mini-batch loss LD(Ï)\n",
    "        disc_loss_real = np.add(discriminator_model.train_on_batch(real_image_batch, y_real), disc_loss_real)\n",
    "        disc_loss_refined = np.add(discriminator_model.train_on_batch(refined_image_batch, y_refined),\n",
    "                                   disc_loss_refined)\n",
    "\n",
    "    if not i % log_interval:\n",
    "        # plot batch of refined images w/ current refiner\n",
    "        figure_name = 'refined_image_batch_step_{}.png'.format(i)\n",
    "        print('Saving batch of refined images at adversarial step: {}.'.format(i))\n",
    "\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "        plot_batch(\n",
    "            np.concatenate((synthetic_image_batch, refiner_model.predict_on_batch(synthetic_image_batch))),\n",
    "            os.path.join(cache_dir, figure_name),\n",
    "            label_batch=['Synthetic'] * batch_size + ['Refined'] * batch_size)\n",
    "\n",
    "        # log loss summary\n",
    "        print('Refiner model loss: {}.'.format(combined_loss / (log_interval * k_g * 2)))\n",
    "        print('Discriminator model loss real: {}.'.format(disc_loss_real / (log_interval * k_d * 2)))\n",
    "        print('Discriminator model loss refined: {}.'.format(disc_loss_refined / (log_interval * k_d * 2)))\n",
    "\n",
    "        combined_loss = np.zeros(shape=len(combined_model.metrics_names))\n",
    "        disc_loss_real = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "        disc_loss_refined = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "\n",
    "        # save model checkpoints\n",
    "        model_checkpoint_base_name = os.path.join(cache_dir, '{}_model_step_{}.h5')\n",
    "        refiner_model.save(model_checkpoint_base_name.format('refiner', i))\n",
    "        discriminator_model.save(model_checkpoint_base_name.format('discriminator', i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0784e5cb-964e-eb52-9739-a3e7591f7375"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
